# -*- coding: utf-8 -*-
"""ds_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NKKyXdDHO142fQsWaQRaDtqzBuWR9jCb

# Title: Customer Segmentation Multiclass Classification Project
### **Name**: Tirth Vanparia
### **RegNo.**: 21BCE2058

# Imports
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('Train.csv')
df.head()

"""- ID	Unique ID
- Gender	Gender of the customer
- Ever_Married	Marital status of the customer
- Age	Age of the customer
- Graduated	Is the customer a graduate?
- Profession	Profession of the customer
- Work_Experience	Work Experience in years
- Spending_Score	Spending score of the customer
- Family_Size	Number of family members for the customer (including the customer)
- Var_1	Anonymised Category for the customer
- Segmentation	(target) Customer Segment of the customer

# Data Cleaning
"""

df

df.duplicated().sum()
# no duplicates

df.isna().sum()

df['Ever_Married'].hist()

df['Ever_Married'].fillna(method='ffill',inplace=True)

df.isna().sum()

print(df['Work_Experience'].value_counts())
sns.histplot(data=df,x='Work_Experience')

#filling the missing Work_Experience data
df['Work_Experience'] = df['Work_Experience'].fillna(method='ffill')

df

df.isna().sum()

plt.figure(figsize=(10,6))
df['Profession'].hist(bins=20)
plt.title('Profession Distribution')

plt.figure(figsize=(10,6))
df['Graduated'].hist(bins=3)
plt.title('Graduated Distribution')

plt.figure(figsize=(10,6))
df['Family_Size'].hist(bins=10)
plt.title('Family Size Distribution')

plt.figure(figsize=(10,6))
df['Var_1'].hist(bins=10)
plt.title('Anonymous Variable Distribution')

#filling the missing data for married, profession, family size and var 1
df['Var_1'] = df['Var_1'].fillna(method='ffill')
df['Family_Size'] = df['Family_Size'].fillna(method='ffill')
df['Graduated'] = df['Graduated'].fillna(method='ffill')
df['Profession'] = df['Profession'].fillna(method='ffill')

#df.drop('Graduted',inplace=True,axis=1)

df.head()

df.info()

"""There are no more null instance in the dataframe. However, the data type of the features are not the same. Three can be spotted right now: float, int and object. The next section will deal with the different datatypes.

# Data Preprocessing
`Data can be categorical or numerical. Not all machine learning algorithms can handle categorical data as inputs. Artificial Neural Networks can only take numerical data as inputs. Therefore, it is a good thing to do to turn the data into numerical data.`
"""

df['Gender'].value_counts()

# Change gender to numeric data
def change_gender(gender):
    if gender == 'Male':
        return 1
    else:
        return 0
df['Gender'] = df['Gender'].apply(change_gender)

categorical_features = ['Ever_Married','Graduated','Profession','Spending_score','Var_1']
df.head()

print(df['Ever_Married'].value_counts(),'\n', df['Graduated'].value_counts())

# change married and graduated data into numeric using lambda function
for i in categorical_features[0:2]:
    df[i] = df[i].apply(lambda x: 1 if x=='Yes' else 0)
df.head()

# Label encoder on Profession feature
from sklearn.preprocessing import LabelEncoder


le = LabelEncoder()
y = le.fit_transform(df['Profession'])
df['Profession'] = y
df.head()

# label encoder for spending score and Var_1
categorical_features = ['Ever_Married','Graduated','Profession','Spending_Score','Var_1']
for i in categorical_features[-2:]:
    le = LabelEncoder()
    y = le.fit_transform(df[i])
    df[i] = y
df.head()

df.drop('ID',inplace=True,axis=1)

df.describe()

"""## Splitting Data"""

from sklearn.model_selection import train_test_split

X = df.drop('Segmentation',axis=1)
y = df['Segmentation']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train.shape,y_train.shape,X_test.shape,y_test.shape

"""##Normalization"""

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
X_train.shape,y_train.shape,X_test.shape,y_test.shape

"""# Training Machine Learning Models"""

from sklearn.linear_model import LogisticRegression # still classificication
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
# Sklearn models: https://scikit-learn.org/stable/supervised_learning.html

"""# Make prediction using the Decision Tree
dt = DecisionTreeClassifier() # Make a decision tree instance
dt.fit(X_train,y_train) #fit the dt instance to the training data and labels
dt_preds = dt.predict(X_test) # make prediction using the dt model using x_test
dt_preds #predicted labels
"""

# Make prediction using 5 machine learning models
dt = DecisionTreeClassifier()
svc = SVC()
knn = KNeighborsClassifier()
nb = GaussianNB()
lr = LogisticRegression()
models = [dt,svc,knn,nb,lr]

preds = [[],[],[],[],[]]

for i,model in enumerate(models):
    model.fit(X_train,y_train)
    preds[i] = model.predict(X_test)
preds

"""##Evaluating the models"""

from sklearn.metrics import accuracy_score,precision_score,f1_score,recall_score,classification_report,confusion_matrix,ConfusionMatrixDisplay

def evaluate_model(predictions,name):
    print('Model: ',name)
    print(classification_report(y_test,predictions))
    cm = confusion_matrix(y_test,predictions)
    disp   = ConfusionMatrixDisplay(cm)
    disp.plot()
    na = 'Conf'
    plt.title('Confusion Matrix for '+ name)

#models = [dt,svc,knn,nb,lr]
for i,y_preds in enumerate(preds):
    evaluate_model(predictions = y_preds, name = str(models[i]).split('()')[0])

"""1. Cleaned the dataset
2. Preprocessed the dataset into numerical data

    a.Split the data into Training and testing dataset
    
    b. Performed normalization
3. Trained 5 machine learning models : Decision Tree, Support Vector Machine, Logitsic Regression, Gaussian Naive Bayes, KNearestNeighbor
4. Evaluated the ML models

The best model so far is **SVM** model with 51% accuracy and 50% percision.

# Improving our best model
"""

svm = SVC()
svm.fit(X_train,y_train)

svm.get_params()

# use GridSearchCV to obtain best parameters
from sklearn.model_selection import GridSearchCV
param_grid = [
  {'C': [1, 10, 100], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},
 ]
svm = SVC()
clf = GridSearchCV(svm,param_grid)
clf.fit(X_train,y_train)

clf.best_estimator_,clf.best_score_,clf.best_params_

cv_results = pd.DataFrame(clf.cv_results_)
cv_results

# Obtained the best SVM SVC with Grid Search
best_param_svm = clf.best_estimator_
best_param_svm.fit(X_train,y_train)

"""# Testing with test.csv

- Import test.csv
- Clean it
- Preprocess it to same type of data
- Normalize it
- Make predictions with the model
"""

test = pd.read_csv('Test.csv')
test

test.duplicated().sum()

test.isnull().sum()

#filling the missing data for married, profession, family size and var 1
#filling the missing Work_Experience data
test['Ever_Married'].fillna(method='ffill',inplace=True)
test['Work_Experience'] = test['Work_Experience'].fillna(method='ffill')
test['Var_1'] = test['Var_1'].fillna(method='ffill')
test['Family_Size'] = test['Family_Size'].fillna(method='ffill')
test['Graduated'] = test['Graduated'].fillna(method='ffill')
test['Profession'] = test['Profession'].fillna(method='ffill')

test.info()

# Change gender to numeric data
def change_gender(gender):
    if gender == 'Male':
        return 1
    else:
        return 0
test['Gender'] = test['Gender'].apply(change_gender)

# label encoder for spending score and Var_1
categorical_features = ['Ever_Married','Graduated','Profession','Spending_Score','Var_1']
for i in categorical_features[-3:]:
    le = LabelEncoder()
    y = le.fit_transform(test[i])
    test[i] = y
test.head()
# change married and graduated data into numeric using lambda function
for i in categorical_features[0:2]:
    test[i] = test[i].apply(lambda x: 1 if x=='Yes' else 0)
test.head()


categorical_features = ['Ever_Married','Graduated','Profession','Spending_score','Var_1']
test.head()

test.drop('ID',axis=1,inplace=True)
test.head()

test_features = test.drop('Segmentation',axis=1)
test_labels = test['Segmentation']

test_features = scaler.transform(test_features)

predictions = best_param_svm.predict(test_features)
predictions

"""## Evaluate the best SVM model's results"""

print(classification_report(test_labels,predictions))

cm = confusion_matrix(test_labels,predictions)
cm

disp = ConfusionMatrixDisplay(cm)
disp.plot()

"""# Conclusion

I cleaned the training dataset, preprocessed it to numeric data types,split into 80% training, 20% testing dataset, normalized the features, made **5 machine learning models**: Decision Tree, Naive Nayes, K Nearest Neighbors, Logistic Regression and Support Vector Machine.
Of the 5 models, **SVM** obtained the best accuracy. Therefore, I hyper tuned SVM using GridSearch.
Using the best parameters, I imported the testing dataset, prepared it the same way I did the training dataset and applied the best SVM model on it.

Unfornately, the results didn't do so well, producing an accuracy of 33% and weighted precsion of 0.31.

Way to have improved the model is by not splitting the training set into train & test set again and simply use the GridSearchCV to find best params.
"""